---
title: Machine Learning Notes
date: '2021-08-15'
tags: ['school', 'machinelearning']
---

Extracting knowledge from a set of data, also known as predictive analytics. Machine learning at its core solves the problem of having handcoded rules for the decision making in software because as the input variation increases the quantity of rules needed isn't humanly possible to implement therefore raising the need for a system to implement new rules automatically.

***

**Machine Learning**

Spam detection is an early example of machine learning since black listing words or phrases is practical up to a point, after which we need an algorithm that can update itself with new rules automatically. Facial recognition is another example, it is a task that we have a hard time recreating since computers process images in a fundamentally different way. Disadvantages of manually crafted rules include:

1. Rules tend to be so specific that changing the input slightly requires the modification of the entire system.
2. Rules are only useful when designed by humans with an incredibly deep understanding of all possible variations.

Supervised learning is a useful machine learning algorithm that generalizes a pattern from a set of known examples, it needs prior knowledge of the inputs and the desired outputs and its capabilities will be as good as its dataset. Examples include handwritten identification, tumor image identification and fraudulent transactions detection.

Unsupervised learning algorithms are harder to understand and evaluate but work even when we don't have a set of expected outputs. Examples include identifying the main theme in a text, segmenting customers into preference groups or detecting abnormal behavior in a system. Datasets are generally formatted in tables for ease of manipulation and filtering, also we can assign each row to a data point or entity and each column to a property or feature.

Feature extraction is the science of extracting the relevant features of a system and its done by carefully filtering the dataset, this is a fundamental step since machine learning requires a clear understanding of how the dataset relates to the task being solved. When starting with a machine learning system answer:

1. What is the question that the system will answer?
2. Is the data collected useful for answering the question?
3. What is the best representation of the question for a machine?
4. Do I have enough and good data?
5. What features of the data did I extracted?
6. Will the features enable the right predictions?
7. How will I measure the application success?
8. How does the system interact with other existing systems?

For this class we will use Python, scikit learn, NumPy (arrays, generators), SciPy (linear algebra, optimization, statistics), Matplotlib (charts) and Pandas (data wrangling, tables).

**First Application**

Suppose a botanist wants image recognition for a set of similar looking flowers, he has length and width of the petals and the sepals in centimeters, he has a dataset with the correct classification of three types of flower. This is a classification problem therefore the system must assign any input to its correct class, in this case there are three classes. For each data point in the dataset there will be a label representing the class it belongs to.

```python
from skylearn.datasets import load_iris
# Bunch datatype, similar to a dict
iris_dataset = load_iris()
print("Keys in Dataset: \n{}".format(iris_dataset.keys()))
# DESCR is a short dataset description
print(iris_dataset['DESCR'][:190])
# Target names is a str array with the classes
print("Target names: \n{}".format(iris_dataset['target_names']))
# Feature names is a str array with the relevant features
print("Feature names: \n{}".format(iris_dataset['feature_names']))
# The data itself is in a numpy array with the 4 measurements
print("Type of data: {}".format(type(iris_dataset['data'])))
# The Rows are the dataset flowers and the Columns are their measurements
print("Shape of data: {}".format(iris_dataset['data'].shape))
# Therefore we have 150 samples and 4 features in the dataset.
print("First five rows: \n{}".format(iris_dataset['data'][:5]))
# Target is a numpy array with the correct label for each sample
print("Shape of target: {}".format(iris_dataset['target'].shape))
# Note that Target is encoded using the index of target names
print("Target: \n{}".format(iris_dataset['target']))
```

Note that we don't have a way of known if the system has generalized a pattern or simply memorized the input, therefore we will split the dataset into two groups, one  to train the model (training set) and the other half to test (testing set) if it has memorized or generalized the data. The function train_test_split is used for dividing the sets with a deterministic pseudorandom number generator to ensure that the testing dataset contains every class.

```python
from skylearn.datasets import load_iris
iris_dataset = load_iris()
# Set the training and testing datasets
# By default it extracts 75 percent as training and 25 percent as testing
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(iris_dataset['data'], iris_dataset['target'], random_state=0)
print("X_train shape: {}".format(X_train.shape))
print("y_train shape: {}".format(y_train.shape))
print("X_test shape: {}".format(X_test.shape))
print("y_test shape: {}".format(y_test.shape))
# Create a datafraim with the X_train and its features
iris_dataframe = pandas.DataFrame(X_train, columns = iris_dataset.feature_names)
# Create a scatter matrix from the dataframe and color by y_train
grr = pandas.scatter_matrix(iris_dataframe, c = y_train, figsize = (15, 15), marker = 'o', hist_kwds={'bins': 20}, s = 60, alpha = 0.8, cmap = mglearn.cm3)
```

We inspect the data to determine if the problem can be solved by Machine Learning, if the desired result is contained in the dataset or if there are peculiarities or abnormalities. Inconsistences and unexpected measurements are common for machine learning models. Plotting can be used as a tool to inspect pairs or features, with pair pairing in the case of a relatively small number of features.

***

TODO:

1. Continue from the book notes.


