<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="/_next/static/css/35cab027331baf7e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/35cab027331baf7e.css" data-n-g=""/><link rel="preload" href="/_next/static/css/f1cf44881b14e9fc.css" as="style"/><link rel="stylesheet" href="/_next/static/css/f1cf44881b14e9fc.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee7e63bc15b31913.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-83cebdb887f48834.js" defer=""></script><script src="/_next/static/chunks/pages/_app-3ca76a984b940bc2.js" defer=""></script><script src="/_next/static/chunks/675-ccede56ccdc0c3ea.js" defer=""></script><script src="/_next/static/chunks/465-4fe278bcc6195523.js" defer=""></script><script src="/_next/static/chunks/803-0cdd1b04f7cffd56.js" defer=""></script><script src="/_next/static/chunks/pages/index-1a05144f89ae5711.js" defer=""></script><script src="/_next/static/DPwb1nK7hmgB_yIPdLqpw/_buildManifest.js" defer=""></script><script src="/_next/static/DPwb1nK7hmgB_yIPdLqpw/_ssgManifest.js" defer=""></script><style id="__jsx-2973409488">.card-layout.jsx-2973409488{display:-webkit-box;display:-webkit-flex;display:-moz-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-webkit-flex-direction:row;-moz-box-orient:horizontal;-moz-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;margin:1vh 11vw 1vh 11vw}.card-container.jsx-2973409488{display:-webkit-box;display:-webkit-flex;display:-moz-box;display:-ms-flexbox;display:flex;-webkit-box-flex:0;-webkit-flex:0 1 24vw;-moz-box-flex:0;-ms-flex:0 1 24vw;flex:0 1 24vw;-webkit-box-orient:vertical;-webkit-box-direction:normal;-webkit-flex-direction:column;-moz-box-orient:vertical;-moz-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;margin:1vh .87vw 1vh .87vw;border-bottom:3px solid transparent}.card-container.jsx-2973409488:hover{border-bottom:3px solid #fb232e;-webkit-transform:scale(.99);-moz-transform:scale(.99);-ms-transform:scale(.99);-o-transform:scale(.99);transform:scale(.99)}.img-container.jsx-2973409488{width:100%;max-width:100%}img.jsx-2973409488{width:100%;max-width:100%;height:12vw;-o-object-fit:cover;object-fit:cover;-webkit-border-radius:5px;-moz-border-radius:5px;border-radius:5px}.info-container.jsx-2973409488{display:-webkit-box;display:-webkit-flex;display:-moz-box;display:-ms-flexbox;display:flex;-webkit-box-flex:100%;-webkit-flex:100%;-moz-box-flex:100%;-ms-flex:100%;flex:100%;max-width:100%;-webkit-box-orient:vertical;-webkit-box-direction:normal;-webkit-flex-direction:column;-moz-box-orient:vertical;-moz-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;word-wrap:break-word;overflow-wrap:break-word}.info-container-link.jsx-2973409488{font-family:"Maven Pro",sans-serif;font-weight:bold;font-size:-webkit-calc(20px + (26 - 20)*((100vw - 300px)/(1600 - 300)));font-size:-moz-calc(20px + (26 - 20)*((100vw - 300px)/(1600 - 300)));font-size:calc(20px + (26 - 20)*((100vw - 300px)/(1600 - 300)));text-decoration:none;word-wrap:break-word;overflow-wrap:break-word;color:#1d2b35}.info-container-link.jsx-2973409488:hover{color:#20a4f3}.info-container-link.jsx-2973409488:active{color:#fb232e}.description-container.jsx-2973409488{display:-webkit-box;display:-webkit-flex;display:-moz-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-webkit-flex-direction:column;-moz-box-orient:vertical;-moz-box-direction:normal;-ms-flex-direction:column;flex-direction:column;max-width:100%}.description.jsx-2973409488{font-family:"Source Sans Pro",sans-serif;font-size:-webkit-calc(15px + (18 - 15)*((100vw - 300px)/(1600 - 300)));font-size:-moz-calc(15px + (18 - 15)*((100vw - 300px)/(1600 - 300)));font-size:calc(15px + (18 - 15)*((100vw - 300px)/(1600 - 300)));color:#071013cc;max-width:100%}.tags-container.jsx-2973409488{display:-webkit-box;display:-webkit-flex;display:-moz-box;display:-ms-flexbox;display:flex;-webkit-box-flex:100%;-webkit-flex:100%;-moz-box-flex:100%;-ms-flex:100%;flex:100%;max-width:100%;-webkit-box-orient:vertical;-webkit-box-direction:reverse;-webkit-flex-direction:column-reverse;-moz-box-orient:vertical;-moz-box-direction:reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse;margin:2vh 0 2vh 0}.tags-container-tag.jsx-2973409488{display:-webkit-box;display:-webkit-flex;display:-moz-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-webkit-flex-direction:row;-moz-box-orient:horizontal;-moz-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap}.tag-link.jsx-2973409488{font-family:"Share Tech Mono",monospace;font-size:-webkit-calc(14px + (16 - 14)*((100vw - 300px)/(1600 - 300)));font-size:-moz-calc(14px + (16 - 14)*((100vw - 300px)/(1600 - 300)));font-size:calc(14px + (16 - 14)*((100vw - 300px)/(1600 - 300)));color:#1d2b35;text-decoration:none;margin-right:1em;border-bottom:1px dashed #fb232e}.tag-link.jsx-2973409488:hover{background:#20a4f3;color:#fffecb;border-bottom:1px dashed #fffecb}.tag-link.jsx-2973409488:active{background:#fb232e}</style><style id="__jsx-3720486262">.header-info.jsx-3720486262{background:#1d2b35}.greetings.jsx-3720486262{padding:2vw 10vw 2vw 10vw}.greetings-heading.jsx-3720486262{font-family:"Ubuntu",sans-serif;font-size:2em;text-align:center;color:#ffaa33}.greetings-statement.jsx-3720486262{font-family:"Ubuntu",sans-serif;font-size:1.5em;padding-top:1vh;text-align:center;color:#ffaa33}.greetings-tags.jsx-3720486262{display:-webkit-box;display:-webkit-flex;display:-moz-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-webkit-flex-direction:row;-moz-box-orient:horizontal;-moz-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-pack:center;-webkit-justify-content:center;-moz-box-pack:center;-ms-flex-pack:center;justify-content:center;padding:1vw 5vw 1vw 5vw}.posts-display-container.jsx-3720486262{padding:1vh 10vw 1vh 10vw}.quote.jsx-3720486262{color:#fb232e;font-family:"Ubuntu",sans-serif;font-size:-webkit-calc(2rem + .5vw);font-size:-moz-calc(2rem + .5vw);font-size:calc(2rem + .5vw)}@media screen and (max-width:920px){.greetings-heading.jsx-3720486262{font-size:20px}.greetings-statement.jsx-3720486262{font-size:18px}.greetings-tags.jsx-3720486262{display:none}.posts-display-container.jsx-3720486262{padding:1vh 5vw 1vh 5vh}}@media screen and (max-width:480px){.greetings-heading.jsx-3720486262{font-size:18px}.greetings-statement.jsx-3720486262{font-size:16px}.greetings-tags.jsx-3720486262{display:none}.posts-display-container.jsx-3720486262{padding:1vh 5vw 1vh 5vh}}</style></head><body><div id="__next"><header class="Header_header__iG0T4"><div class="flex items-center h-16 px-4"><h2><a class="font-bold text-2xl flex-1" href="/">PressBlog</a></h2><ul><li><a href="/about">About</a></li><li><a href="/#">GitHub Code</a></li></ul></div></header><main><div class="p-4 flex justify-center"><div class="jsx-3720486262"><div class="sm:grid sm:grid-cols-2 lg:grid-cols-3 xl:grid-cols-4 2xl:grid-cols-4 max-w-6xl mx-auto py-4"><div class="jsx-2973409488 cursor-pointer sm:p-3 sm:hover:shadow-slate-400 sm:shadow-md rounded-lg sm:border sm:border-slate-400 sm:m-2 transition-shadow duration-200 group"><a href="/blog/super-fast-python-numba"><img alt="image is not available" loading="lazy" width="500" height="300" decoding="async" data-nimg="1" class="sm:rounded-t-lg group-hover:opacity-80 transition-opacity duration-200" style="color:transparent;max-width:100%;height:auto;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http%3A//www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 500 300&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage preserveAspectRatio=&#x27;none&#x27; filter=&#x27;url(%23b)&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; height=&#x27;100%25&#x27; width=&#x27;100%25&#x27; href=&#x27;/spinner.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/images/super-fast-python-numba/super-fast-python-numba.jpg"/><div class="jsx-2973409488 p-2"><h2 class="jsx-2973409488 text-lg font-bold">Super fast Python (Part-5): Numba</h2><p class="jsx-2973409488 line-clamp-2 text-md">Speed up Numerical computations and functions in Python with Numba and Numpy.</p><p class="jsx-2973409488 flex items-center">January, 6th. 2023</p></div><div class="jsx-2973409488 tags-container"><div class="jsx-2973409488 tags-container-tag"><a class="jsx-2973409488 tag-link" href="/tags/python-performance">#python-performance</a></div></div></a></div><div class="jsx-2973409488 cursor-pointer sm:p-3 sm:hover:shadow-slate-400 sm:shadow-md rounded-lg sm:border sm:border-slate-400 sm:m-2 transition-shadow duration-200 group"><a href="/blog/super-fast-python-cython"><img alt="image is not available" loading="lazy" width="500" height="300" decoding="async" data-nimg="1" class="sm:rounded-t-lg group-hover:opacity-80 transition-opacity duration-200" style="color:transparent;max-width:100%;height:auto;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http%3A//www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 500 300&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage preserveAspectRatio=&#x27;none&#x27; filter=&#x27;url(%23b)&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; height=&#x27;100%25&#x27; width=&#x27;100%25&#x27; href=&#x27;/spinner.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/images/super-fast-python-cython/super-fast-python-cython.jpg"/><div class="jsx-2973409488 p-2"><h2 class="jsx-2973409488 text-lg font-bold">Super fast Python (Part-4): Cython</h2><p class="jsx-2973409488 line-clamp-2 text-md">Convert slow Python code to run as fast as C/C++ using Cython.</p><p class="jsx-2973409488 flex items-center">Nov 18, 2022</p></div><div class="jsx-2973409488 tags-container"><div class="jsx-2973409488 tags-container-tag"><a class="jsx-2973409488 tag-link" href="/tags/python-performance">#python-performance</a></div></div></a></div><div class="jsx-2973409488 cursor-pointer sm:p-3 sm:hover:shadow-slate-400 sm:shadow-md rounded-lg sm:border sm:border-slate-400 sm:m-2 transition-shadow duration-200 group"><a href="/blog/super-fast-python-good-practices"><img alt="image is not available" loading="lazy" width="500" height="300" decoding="async" data-nimg="1" class="sm:rounded-t-lg group-hover:opacity-80 transition-opacity duration-200" style="color:transparent;max-width:100%;height:auto;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http%3A//www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 500 300&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage preserveAspectRatio=&#x27;none&#x27; filter=&#x27;url(%23b)&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; height=&#x27;100%25&#x27; width=&#x27;100%25&#x27; href=&#x27;/spinner.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/images/super-fast-python-good-practices/super-fast-python-good-practices.jpg"/><div class="jsx-2973409488 p-2"><h2 class="jsx-2973409488 text-lg font-bold">Super fast Python (Part-2): Good Practices</h2><p class="jsx-2973409488 line-clamp-2 text-md">Write Python programs by following good practices to run code incredibly faster.</p><p class="jsx-2973409488 flex items-center">Nov 9, 2022</p></div><div class="jsx-2973409488 tags-container"><div class="jsx-2973409488 tags-container-tag"><a class="jsx-2973409488 tag-link" href="/tags/python-performance">#python-performance</a></div></div></a></div><div class="jsx-2973409488 cursor-pointer sm:p-3 sm:hover:shadow-slate-400 sm:shadow-md rounded-lg sm:border sm:border-slate-400 sm:m-2 transition-shadow duration-200 group"><a href="/blog/super-fast-python-why-python-slow"><img alt="image is not available" loading="lazy" width="500" height="300" decoding="async" data-nimg="1" class="sm:rounded-t-lg group-hover:opacity-80 transition-opacity duration-200" style="color:transparent;max-width:100%;height:auto;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http%3A//www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 500 300&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage preserveAspectRatio=&#x27;none&#x27; filter=&#x27;url(%23b)&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; height=&#x27;100%25&#x27; width=&#x27;100%25&#x27; href=&#x27;/spinner.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/images/super-fast-python-why-python-slow/super-fast-python-why-python-slow.jpg"/><div class="jsx-2973409488 p-2"><h2 class="jsx-2973409488 text-lg font-bold">Super fast Python (Part-1): Why Python is Slow?</h2><p class="jsx-2973409488 line-clamp-2 text-md">Why Python is slow compared to C/C++ or Java? And where can we improve our code to run fast?</p><p class="jsx-2973409488 flex items-center">Nov 7, 2022</p></div><div class="jsx-2973409488 tags-container"><div class="jsx-2973409488 tags-container-tag"><a class="jsx-2973409488 tag-link" href="/tags/python-performance">#python-performance</a></div></div></a></div><div class="jsx-2973409488 cursor-pointer sm:p-3 sm:hover:shadow-slate-400 sm:shadow-md rounded-lg sm:border sm:border-slate-400 sm:m-2 transition-shadow duration-200 group"><a href="/blog/what-is-react-native"><img alt="image is not available" loading="lazy" width="500" height="300" decoding="async" data-nimg="1" class="sm:rounded-t-lg group-hover:opacity-80 transition-opacity duration-200" style="color:transparent;max-width:100%;height:auto;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http%3A//www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 500 300&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage preserveAspectRatio=&#x27;none&#x27; filter=&#x27;url(%23b)&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; height=&#x27;100%25&#x27; width=&#x27;100%25&#x27; href=&#x27;/spinner.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/images/no-image.png"/><div class="jsx-2973409488 p-2"><h2 class="jsx-2973409488 text-lg font-bold">What is React Native?</h2><p class="jsx-2973409488 line-clamp-2 text-md"></p><p class="jsx-2973409488 flex items-center">Mar 23, 2022</p></div><div class="jsx-2973409488 tags-container"><div class="jsx-2973409488 tags-container-tag"></div></div></a></div></div><button class="jsx-3720486262 Home_button__69vwW">Load more</button></div></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postsMetaData":[{"metadata":{"title":"Super fast Python (Part-5): Numba","description":"Speed up Numerical computations and functions in Python with Numba and Numpy.","imgName":"super-fast-python-numba/super-fast-python-numba.jpg","date":"January, 6th. 2023","tags":["python-performance"],"keywords":["numba","python-performance'","python-optimize","python","fast-python","speed","jit","numba-numpy"],"id":"super-fast-python-numba"},"content":"\n![Super fast Python: Numba](super-fast-python-numba/super-fast-python-numba.jpg)\n\n# Super fast Python (Part-5): Numba\n\nThis is the fifth and last post in the series on Python performance and Optimization. The series points out the utilization of inbuilt libraries, low-level code conversions, and other Python implementations to speed-up Python. The other posts included in this series are \n\n- (Part-1): [Why Python is slow?](https://santhalakshminarayana.github.io/blog/super-fast-python-why-python-slow)\n- (Part-2): [Good practices to write fast Python code](https://santhalakshminarayana.github.io/blog/super-fast-python-good-practices)\n- (Part-3): [Multi-processing in Python](https://santhalakshminarayana.github.io/blog/super-fast-python-multi-processing)\n- (Part-4): [Use Cython to get speed as fast as C](https://santhalakshminarayana.github.io/blog/super-fast-python-cython)\n- (Part-5): Use Numba to speed up Python Functions (this post)\n\nIn the last post about [Cython to speed-up Python code](https://santhalakshminarayana.github.io/blog/super-fast-python-cython), we discussed writing Python code in C-style, compiling that code separately into an object file, and using that generated file as an import directly into Python. But, not all people would feel comfortable writing C-style code or even some might not know C at all. So, to deal with such cases and get the performant efficient code to speed-up Python, one can use [Numba](https://numba.pydata.org/) instead. Numba translates Python code to machine code that executes almost as fast as C/C++ if optimized correctly.\n\n## What is Numba?\n\nNumba is a JIT (just-in-time) compiler that takes Python byte code and compiles it into machine code directly using [LLVM](https://llvm.org/) compiling mechanism. JIT is a type of interpreter that compiles frequently called code into machine code and caches that generated machine code to be used later for faster execution type. Here, Numba also takes Python code and generated machine code which the Python interpreter calls directly instead of interpreting and converting to machine code each time. Numba works best for numerical calculations, Array and Numpy operations, and loops. With Numba, we can write vectorized operations and parallelized loops to run on either CPU or GPU. \n\nNumba decorators are one of the many ways to invoke the JIT compilation. Numba provides different decorators to compile code in different modes and types, the common decorators used in Numba are:\n- @jit - invoke JIT compilation for the provided function\n- @njit - @jit decorator with enabling strict no-python mode\n- @vectorize - convert normal functions into Numpy like **ufuncs**\n- @guvectorize - generalized **ufuncs** for higher dimensional arrays\n- @stencil - make a function behave as a kernel for a **stencil** like operation\n\nNumba also provides different options to pass for some of these decorators to configure the JIT compilation behavior\n- nopython\n- parallel\n- cache\n- nogil\n- fastmath\n- boundscheck\n- error_model\n- cuda\n\n## Numba @jit\n\n**@jit** decorator takes the Python function that needs to machine code compiled. When we make a call to the function we provided to **@jit**, upon the first time calling, Numba compiles the function, caches the machine code, and this machine code is directly used for the execution. As compilation takes time, the first-time call to the function gives some latency. But, for consecutive function calls in the same runtime, just the cached machine code is used instead of re-compiling every time. \n\nLet's consider the following simple function *solve_expression* as an example. *solve_expression* takes some arguments, checks some conditions, and calculates the final polynomial expression. \n\n```python\ndef solve_expression(x, a, b, c, d):\n    A, B, C, D = a, b, c, d\n    if a \u003e 10.1:\n        A = 2 * a\n    if 2.6 \u003c= b \u003c 8.3:\n        B = b - 1/b\n    if c \u003e 4.5:\n        C = 4\n    if d \u003c 9.0:\n        D = d ** 2\n    \n    return A*(x**3) + B*(x**2) + C*(x) + D\n```\n\nNow, use the **@jit** decorator to compile this function into machine code as \n\n```python\nfrom numba import jit\n\n@jit\ndef solve_expression(x, a, b, c, d):\n    A, B, C, D = a, b, c, d\n    if a \u003e 10.1:\n        A = 2 * a\n    if 2.6 \u003c= b \u003c 8.3:\n        B = b - 1/b\n    if c \u003e 4.5:\n        C = 4\n    if d \u003c 9.0:\n        D = d ** 2\n    \n    return A*(x**3) + B*(x**2) + C*(x) + D\n```\n\nIn the code snippet, we have imported the **jit** function decorator and decorated *solve_expression* with it.\n\n```python\nx, a, b, c, d = 2, 13, 1.2, 4, 7\n\nres = solve_expression(x, a, b, c, d)\n``` \n\n\u003e Inspect the Intermediate Representation (IR) of the function using solve_expression.inspect_types()\n\nAs Numba **@jit** defers the JIT compilation until it encounters the first call to the function, the function call with arguments *solve_expression(x, a, b, c, d)* takes some time for executions. But, function calls later at this point will be fast. \n\nNow compare the speeds of the normal Python function and Numba JIT decorated function. Using *solve_expression.py_func()*, we can invoke the normal python function of this JIT decorated function.\n\n```python\n%% timeit\nres = solve_expression.py_func(x, a, b, c, d)\n\n'''Output\n912 ns ± 3.47 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n'''\n```\n\nThe normal Python function takes approx. 900 nanoseconds. \n\n```python\n%%timeit\nres = solve_expression(x, a, b, c, d)\n\n'''Output\n277 ns ± 1.36 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n'''\n```\n\nAnd the Numba version takes approx. 280 nanoseconds. The Numba version is 3x times faster than the pure Python function.\n\n### Compilation options\n\nFor **@jit** decorator, we can pass multiple options to configure the compilation behavior\n\n- **nopython**: if True, enables no-python mode making code execution without Python interpreter interference\n- **nogil**: if True, releases the GIL (only when *nopython=True*) inside the compiled function, useful for concurrent execution such as threads\n- **cache**: if True, store the compiled code in local storage and use this code whenever the function is called instead of re-compiling for every runtime\n- **parallel**: if True, enables automatic parallelization\n- **fastmath**: if True, uses faster math operations but less safe floating-point operations\n\nApart from these, there are some more options available. You can check all options at [@jit reference](https://numba.readthedocs.io/en/stable/reference/jit-compilation.html#jit-functions).\n\n## Lazy and Eager compilation\n\nNote that, as Python function arguments can take any time of arguments, Numba compiles the function for that specific type of the argument passed. If a new type of argument is passed to the function while calling, Numba re-compiles the code for that specific type.\n\n### Lazy compilation\n\nThe above *solve_expression()* function takes any type of argument. If we provide a function like this Numba calculates the optimization steps to be done based on the argument types provided at the first function call. In this way, Numba infers the argument types and compiles the specific version of the same function for different types. \n\nEx: if we change the argument types like this\n\n```python\n# previous values, x, a, b, c, d = 2, 13, 1.2, 4, 7\n# new values\nx, a, b, c, d = 3.9, 12, 5, 9.1, 14\n\nres = solve_expression(x, a, b, c, d)\n```\n\nAs the previous compiled function expects types for *x=int, a=int, b=float, c=int, d=int*, and in the latest function call we have changed some argument types. So, Numba re-compiles the function for new argument types. This mode of the compilation of code is called lazy compilation because Numba compiles for specific argument types only if it encounters them.\n\n### Eager compilation\n\nFor function overloading, we can specify function signatures with argument types and return types in a list with the least significant precision at the top. [Numba types](https://numba.readthedocs.io/en/stable/reference/types.html) follow Numpy convention types with different precision levels.\n\n```python\n@jit(['int32(int32, int32)',\n      'i4(int32, int64)', \n      '(f4, f8)',\n      'f8(f4, f4)'])\ndef func1(a, b):\n    return a + b\n```\n\nIn the above function, we passed function signatures as a list of strings. The syntax is return type is specified first and argument types are specified after. It is allowed to have no return type specified. Numba will infer the return type automatically and use that specification. Calling the function with argument types not provided in the list raises an error.\n\n## @njit or @jit(nopython=True)\n\nNumba **@jit** operates in two modes *nopython* and *object*. In **nopython** mode, no interference of the Python interpreter is required and execution is very fast compared to normal mode. **object** mode is the same as calling function without **@jit**. \n\nNormally with the **@jit** decorator, Numba tries to compile in **nopython** mode. If any part of the code cannot be compiled due to the presence of code that is not supported by Numba like a heterogenous dictionary, some string methods, etc, then Numba fallbacks to **object** or normal Python mode for compilation. But still, it will improve the performance when loops are involved. If there is no code to optimize, **@jit** in object mode runs slower than the normal Python version as Numba compilation involves several function call overheads.\n\nWe can enable the strict *nopython* mode by passing the option to **@jit** as *@jit(nopython=True)*. Numba also provides a separate decorator for this option. *@njit* decorator is an alias for *@jit(nopython=True)*.\n\n```python\n@jit(nopython=True)\ndef f(a, b):\n\treturn a + b\n\n# or\n\n@njit\ndef f(a, b):\n\treturn a + b\n```\n\nWith **nopython** mode, if any code is present that requires object mode compilation, Numba will raise an error. The primary goal is to write functions that can be implemented in strict no-python mode.\n\n---\n\n## Numba and Numpy\n\nNumba is best for Numpy arrays and supports some [Numpy features](https://numba.readthedocs.io/en/stable/reference/numpysupported.html) in no-python mode.\n\n```python\n@njit(['int64[:](int64[:, :], int64[:, :])'])\ndef f(a, b):\n    c = np.empty(a.shape[0], dtype='int64')\n    \n    for i in range(a.shape[0]):\n        c[i] = a[i].sum() * b[i].sum()\n    \n    return c\n```\n\nThe function takes 2 2D Numpy int64 arrays as arguments with the return type as an array of float64. The function calculates the sum of the product of the sum of each row of *a* and *b*.\n\n```python\nx, y, n = 1000, 1000, 1_000_000\nl, h = 0, 100\na = np.random.randint(l, h, n).reshape(x, y)\nb = np.random.randint(l, h, n).reshape(x, y)\n```\n\nIf we compare both normal Numpy calculation of the above function and Numba compiled code, \n\n```python\n%%timeit\n# normal Python calculation\nres = a.sum(axis=1) * b.sum(axis=1)\n\n'''Output:\n1.4 ms ± 7.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n'''\n```\n\n```python\n%%timeit \n# Numba compiled function\nres = f(a, b)\n\n'''Output:\n1.19 ms ± 15.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n'''\n```\n\nNumba compiled version takes **1.19 ms** which is slightly faster than normal calculation with Numpy **1.4ms**.\n\n## @vectorize decorator\n\nOne of the main reasons for Numpy speed is that most of the Numpy functions are [ufunc](https://numpy.org/doc/stable/user/basics.ufuncs.html)s (universal functions) that are vectorized and implemented inside compiled layer of Numpy and hence the speed. We might run into a situation where we cannot find any existing Numpy functions for use and write a workaround by combining Numpy functions into a single operation. This new operation is not optimized and we might lose the speed that Numpy provides due to custom loops. To solve this problem, we can make any function as **ufunc** that comes with vectorization and speed. \n\nWith [@vectorize](https://numba.readthedocs.io/en/stable/user/vectorize.html), Numba provides functionality to create custom vectorized universal functions. The universal function takes scalar values and returns a scalar value and these functions are applied over any Numpy arrays where array values are passed as single scalar values and an outside loop is automatically generated.\n\n```python\n@vectorize(['float64(int64, int64)'])\ndef v_expr(a, b):\n    return (a**2) + (a*3) + (b/2) + 10\n```\n\nHere, we created a simple universal function that takes two scalar values and returns a calculated expression. \n\n```python\nn = 1_000_000\nl, h = 0, 100\na = np.random.randint(l, h, n)\nb = np.random.randint(l, h, n)\n```\n\nThe speed comparison of the Numpy expression and vectorized ufunc function gives \n\n```python\n%%timeit\n# general way of calculating Numpy expression\nres = (a**2) + (a*3) + (b/2) + 10\n\n'''Output:\n8.66 ms ± 71.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n'''\n```\n\n```python\n%%timeit\n# calculate expression with ufunc\nres = v_expr(a, b)\n\n'''Output:\n2.17 ms ± 96.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n'''\n```\n\nthe optimized function **v_expr** takes **2.17 ms** which is 4x faster than the normal calculation with Numpy.\n\n---\n\nIn this blog, we discussed basic Numba decorators and compared Numba compiled functions with normal Python functions. Explore other features of Numba like \n- [automatic parallelization](https://numba.readthedocs.io/en/stable/user/parallel.html)\n- [loop parallelization with prange](https://numba.readthedocs.io/en/stable/user/parallel.html#explicit-parallel-loops)\n- [stencil kernel implementation for arrays](https://numba.readthedocs.io/en/stable/user/stencil.html)\n- [ahead-of-time compilation](https://numba.readthedocs.io/en/stable/user/pycc.html)\n\nRemember that not every function can be passed to Numba as there are some limitations like \n- Numba only supports a subset of Python code like classes, multi-dimensional dictionaries, etc, which are not supported yet. \n- As object mode compilation takes more time than the normal Python mode in some cases, it is better to check the speed of both normal and compiled code execution speed.\n- Support for external libraries like Pandas is not supported.\n- As Numba re-implements some Numpy APIs, there may be different behavior expected.\n\n---\n\n### References\n- [https://github.com/ContinuumIO/gtc2020-numba](https://github.com/ContinuumIO/gtc2020-numba)\n- [https://www.nvidia.com/en-us/glossary/data-science/numba/](https://www.nvidia.com/en-us/glossary/data-science/numba/)\n- [https://www.chrisvoncsefalvay.com/2019/03/23/jit-fast/](https://www.chrisvoncsefalvay.com/2019/03/23/jit-fast/)\n- [https://towardsdatascience.com/numba-weapon-of-mass-optimization-43cdeb76c7da](https://towardsdatascience.com/numba-weapon-of-mass-optimization-43cdeb76c7da)\n- [https://www.infoworld.com/article/3622013/speed-up-your-python-with-numba.html](https://www.infoworld.com/article/3622013/speed-up-your-python-with-numba.html)\n- [https://coderzcolumn.com/tutorials/python/numba](https://coderzcolumn.com/tutorials/python/numba)\n- [https://towardsdatascience.com/numpy-ufuncs-the-magic-behind-vectorized-functions-8cc3ba56aa2c](https://towardsdatascience.com/numpy-ufuncs-the-magic-behind-vectorized-functions-8cc3ba56aa2c)"},{"metadata":{"title":"Super fast Python (Part-4): Cython","description":"Convert slow Python code to run as fast as C/C++ using Cython.","imgName":"super-fast-python-cython/super-fast-python-cython.jpg","date":"Nov 18, 2022","tags":["python-performance"],"keywords":["cython","python-performance'","python-optimize","python","fast-python","speed","python"],"id":"super-fast-python-cython"},"content":"\n![Super fast Python: Cython](super-fast-python-cython/super-fast-python-cython.jpg)\n\n# Super fast Python (Part-4): Cython\n\nThis is the fourth post in the series on Python performance and Optimization. The series points out the utilization of inbuilt libraries, low-level code conversions, and other Python implementations to speed-up Python. The other posts included in this series are \n\n- (Part-1): [Why Python is slow?](https://santhalakshminarayana.github.io/blog/super-fast-python-why-python-slow)\n- (Part-2): [Good practices to write fast Python code](https://santhalakshminarayana.github.io/blog/super-fast-python-good-practices)\n- (Part-3): [Multi-processing in Python](https://santhalakshminarayana.github.io/blog/super-fast-python-multi-processing)\n- (Part-4): Use Cython to get speed as fast as C (this post)\n- (Part-5): [Use Numba to speed up Python Functions and Numeric calculations](https://santhalakshminarayana.github.io/blog/super-fast-python-numba)\n\nIn the last post, we discussed **multiprocessing** to optimize Python code by utilizing parallel computing with multiple CPU cores. Multi-processing is useful when we can split certain parts of code into parallel tasks and then execute them parallelly. \n\nImagine when we don't have any parallelizable code and it is taking huge time in Python compared to **C/C++**, how to optimize the code then? One thing we can do is to convert the Python code into a low-level programming language like C/C++ and [embed that C/C++ code in Python](https://docs.python.org/3/extending/extending.html). Understanding and writing C/C++ code is hard for someone who is not familiar with C/C++. Fortunately, we can get the performance speed as fast as C/C++ by writing the Python code in [Cython](https://cython.org/) which is a superset of Python that provides functionality to write C-Extensions for Python.\n\n## How Cython can improve Speed?\n\nIn our previous discussion on [Why Python is slow?](https://santhalakshminarayana.github.io/blog/super-fast-python-why-python-slow), we learned that the major speed issues arise in Python due to\n\n- interpretation of generated bytecode\n- dynamic types and their management\n\nWith Cython, we can take care of the above problems with both compiled code instead of interpretation and static typing instead of dynamic typing. \n\nCython translates the Python code into C extension code and compiles the C code into an object code that can be imported directly into Python. Also, we can make some changes like adding static types that improve the execution speed drastically over dynamic typing. Cython also supports the usage of C/C++ libraries and functions that are super-fast compared to Python libraries and functions.\n\n### Install Cython\n\n[Install the Cython](https://cython.readthedocs.io/en/stable/src/quickstart/install.html) with Pip as follows\n\n```bash\npip install Cython\n```\n\n\u003e To compile the C code into an object file, we need a C compiler. Ubuntu comes with **gcc** by default. For other platforms like Windows, install the C compiler if not installed previously.\n\n---\n\n## Cython usage\n\nThere are multiple ways to use Cython like building manually,  using in Jupyter as an extension, or importing directly like a Python module without compilation using *pyximport*.\n\n### Build a Cython module manually\n\nWe write the Cython code in a file with the extension *.pyx* instead of the normal Python extension *.py*.\n\nThe compiler translates '.pyx' Cython file into a '.c' C file and then compiles that C file to a sharable object file '.so' (or '.pyd' on Windows). We tell the compiler those build instructions and compilation options by writing a [setup.py](https://cython.readthedocs.io/en/stable/src/userguide/source_files_and_compilation.html#basic-setup-py) file.\n\n### Cython compilation\n\n- Translate the *.pyx* source code to a *.c* file with additional wrappers of Python extension code.\n- Compile the *.c* file with a C compiler to a platform-specific shared object file *.so* that can be imported directly into Python.\n\n```python:calculate_y_cython.pyx\ndef fx(x, a, b, c):\n    d = (a + b) * c\n    return a*x + b*(x**2) + c*(x**3) + d\n\ndef y(x, n, a, b, c):\n    af = fx(x, a, b, c)**2\n    k = abs(n//2 - x)\n    bf = fx(k, a, b, c)\n    return (af-bf)/(n-1 + 1e-12)\n\ndef calculate_y_cython(n):\n    ys = []\n    a, b, c = 2, 5, -4\n    for i in range(n):\n        ys.append(y(i, n, a, b, c))\n    \n    return ys\n```\n\nThe above file *calculate_y.pyx* contains Cython code that looks the same as Python without any optimizations. Now to compile the above file, write a *setup.py* file as following\n\n```python:setup.py\nfrom distutils.core import setup\nfrom Cython.Build import cythonize\n\nsetup(\n    name='Calculate Expression Y',\n    ext_modules=cythonize(\n        \"calculate_y_cython.pyx\", \n        compiler_directives={\"language_level\": \"3\"},\n    )\n)\n```\n\nIn the above *setup.py*, for the **setup()**, we have passed the optional name to our Cython module for *name* and the path to the *.pyx* file for the *ext_module* parameter.\n\nBuild the sharable object file '.so' using the following command in the terminal\n\n```bash\npython setup.py build_ext --inplace\nor\npython setup.py build_ext --inplace --quiet\n```\n\nThis will generate a translated '.c' C file and a compiled '.so' file.\n\nWe can import the above compiled *calculate_y_cython* module directly into Python runtime.\n\n```python:main.py\nfrom time import perf_counter\nfrom calculate_y_cython import calculate_y_cython\n\ndef fx(x, a, b, c):\n    d = (a + b) * c\n    return a*x + b*(x**2) + c*(x**3) + d\n\ndef y(x, n, a, b, c):\n    af = fx(x, a, b, c)**2\n    k = abs(n//2 - x)\n    bf = fx(k, a, b, c)\n    return (af-bf)/(n-1 + 1e-12)\n\ndef calculate_y_py(n):\n    ys = []\n    a, b, c = 2, 5, -4\n    for i in range(n):\n        ys.append(y(i, n, a, b, c))\n    \n    return ys\n\ndef main():\n    n = 100000\n    # Python implementation\n    atime = perf_counter()\n    res = calculate_y_py(n)\n    print(f'Python Time: {perf_counter()-atime:.2}')\n    \n    atime = perf_counter()\n    res = calculate_y_cython(n)\n    print(f'Cython Time: {perf_counter()-atime:.2}')\n    \nif __name__==\"__main__\":\n    main()\n\n\"\"\"Output:\nPython Time: 0.19\nCython Time: 0.16\n\"\"\"\n```\n\n\u003e At the time of writing, the latest Python version is Python 3.11 which is incredibly faster (30-60% in some cases) than earlier versions. So, to understand the Cython potential, I'm running the scripts in Python 3.8.10 on my old system with 8GB Intel(R) i5-8250U 1.60GHz CPU on HP Laptop 15-da0xxx.\n\nIn the above *main.py*, we have imported the *calculate_y_cython* module at line 2. If we look at the output to check the time taken for normal Python implementation and Cython version, they are **0.19** and **0.16** seconds respectively. The time difference is very low and we didn't gain much from Cython because we haven't done any optimization steps like \n\n- static typing\n- limit calling Python's libraries\n- reducing Python's PyObject usage\n\n### Cython Annotations\n\nCython provides an easy way to check where we can optimize our Cython code by using Cython annotate feature. With the following command (*-a* denotes annotate and *-3* denotes **language_level** which is Python3), cython generates an HTML file that we can open in the browser to check for optimizable code. Another option is to pass the **annotate=True** parameter to *cythonize()* function call in **ext_modules** in *setup*.\n\n```bash\ncython -a calculate_y_cython.pyx -3\n```\n\nIf we open the generated HTML file in the browser, it will look like this\n\n![Cython Annotation:=:80](super-fast-python-cython/cython-annotations.png)\n\nThe more yellow lines the more interaction with the Python interpreter. Our goal should be converting as many yellow lines as to white lines that denote pure Cython code. We discuss the optimization part in a later section.\n\n### Cython as an extension in Jupyter\n\nCython can be imported and used in Jupyter directly as an extension without the need for any additional build/compilation steps. \n\nFirst load the Cython extension using *%load_ext cython*, and then, for the cell that is to be Cythonized, use the magic command *%%cython* at the top of that cell as shown in the following image.\n\n![Cython Jupyter:=:40](super-fast-python-cython/cython-jupyter.png)\n\nWe can show the interactive Cython annotations in Jupyter just like we have generated the HTML file above. To show annotations, pass annotate option to *%%cython -a* magic command.\n\n### Import '.pyx' using pyximport\n\nWhile developing or debugging, for each change in the '.pyx' file, running *setup.py* is a repetitive task and cumbersome. Instead, we can dynamically import '.pyx' to Python directly using [pyximport](https://cython.readthedocs.io/en/stable/src/userguide/source_files_and_compilation.html#pyximport) without any external build and compilation. *pyximport* takes care of compiling and building in the background without calling *cythonize()* internally. So, while importing the '.pyx' file, it will take some time to be imported as a regular Python module.\n\n```python\nimport pyximport\npyximport.install(language_level=3)\n\nfrom calculate_y_cython import calculate_y_cython\n```\n\nThough it is easy to work with Cython using **pyximport**, there are some [limitations with pyximport](https://cython.readthedocs.io/en/stable/src/userguide/source_files_and_compilation.html#limitations) and it is also not flexible as normal setup.\n\n\u003e It is not recommended to use **pyximport** while distributing Python packages and modules.\n\n---\n\n## Optimize Cython\n\nIn the previous section, we have seen in the Cython annotations HTML file that many areas in the code need to be optimized for more speed. There are several ways to improve speed like\n\n- define static types\n- use C-libraries and functions\n- utilize OpenMP for parallel computing\n\n### Define static types to variables\n\nSince Python is a dynamic typing language, we can define C-like data types for Python variables in Cython. To declare C variables, prefix the **cdef** keyword to the variable declaration which is the same as declaring variables in C.\n\nThe syntax for variable declaration is\n\n```python\ncdef type variable_name = initilization_value {optional}\n```\n\nThe **type** can be any of the acceptable C data types.\n\n### Define function definitions\n\nPython functions are defined using **def** and [C functions in Cython](https://cython.readthedocs.io/en/latest/src/userguide/language_basics.html#python-functions-vs-c-functions) are defined with the keyword **cdef**. The difference between **def** and **cdef** is that with the former declaration, it can be called from anywhere both local and external modules where as **cdef** functions are only module level. Also, Cython wraps the **def** function that is defined inside '.pyx' into a Python object. There is another declaration called **cpdef** which behaves as **def** when called from outside the module and behaves as **cdef** inside the module, so it is faster inside the same module function call.\n\nCython also provides support for writing function definitions just like C. We can define parameter types and return types. \n\n```python\ncdef(or cpdef) type function_name(type parameter1, ...)\n```\n\nIf no type is specified, the parameters and return values are treated as Python objects which need Python interpretation and they are slow.\n\nNow, make some changes to *calculate_y_cython.pyx* with static type declarations and function definitions as,\n\n\u003e [Call the C functions in Cython](https://cython.readthedocs.io/en/stable/src/tutorial/external.html) inplace of Python functions that reduces the Python interaction.\n\n```python:calculate_y_cython.pyx\nctypedef long int li\nctypedef long long int lli\n\ncdef lli fx(li x, int a, int b, int c):\n    cdef int d = (a + b) * c\n    return a*x + b*(x**2) + c*(x**3) + d\n\ncdef double y(li x, li n, int a, int b, int c):\n    cdef:\n        lli af = fx(x, a, b, c)**2\n        li k = abs(n//2 - x)\n        lli bf = fx(k, a, b, c)\n    return (af-bf)/(n-1 + 1e-12)\n\ncpdef calculate_y_cython(li n):\n    ys = []\n    cdef:\n        int a = 2, b = 5, c = -4\n        li i = 0\n    for i in range(n):\n        ys.append(y(i, n, a, b, c))\n    \n    return ys\n```\n\n\u003e speed can be improved further by disabling bound checking(@cython.boundscheck(False)) and negative indexing(@cython.wraparound(False)) [compiler directive instructions](https://cython.readthedocs.io/en/stable/src/userguide/source_files_and_compilation.html#compiler-directives).\n\nCheck the annotations for the Cython code, most of the yellow lines are now changed to white and only list operations are dark yellow because lists are Python objects. In a later section, we discuss optimizing lists using C arrays.\n\n![Optimized Cython Annotation:=:70](super-fast-python-cython/optimized_cython.jpg)\n\nIf we check the time for the optimized Cython code,\n\n```python\n%%timeit -n 100\nn = 100000\nres = calculate_y_cython(n)\n\n\"\"\"Output:\n2.04 ms ± 166 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\"\"\"\n```\n\nit takes **0.002** seconds which is approximately **100x** times faster than the normal Python version that takes **0.19** seconds. By adding only static type and some tweaks, we made Python 100x faster. The powers of Cython are not limited to only static typing. We can further speed up the code with Numpy.\n\n---\n\n## 1000x times faster with MemoryView and Numpy\n\nIn the Cython annotation snippet above, we see that the list operations of **ys** are still yellow as *list()* is a python object and Cython cannot optimize Python objects interaction. \n\nTo solve this, we can convert the list type to [memoryview arrays](https://cython.readthedocs.io/en/stable/src/tutorial/array.html), and operations like *append()* to indexing just like looping in C.\n\n```python:calculate_y_cython.pyx\ncimport cython\nimport numpy as np\ncimport numpy as np\n\nctypedef long int li\nctypedef long long int lli\n\n@cython.boundscheck(False)  # Deactivate bounds checking\n@cython.wraparound(False)   # Deactivate negative indexing\ncdef lli fx(li x, int a, int b, int c):\n    cdef int d = (a + b) * c\n    return a*x + b*(x**2) + c*(x**3) + d\n\n@cython.boundscheck(False)\n@cython.wraparound(False)\ncdef double y(li x, li n, int a, int b, int c):\n    cdef:\n        lli af = fx(x, a, b, c)**2\n        li k = abs(n//2 - x)\n        lli bf = fx(k, a, b, c)\n    return (af-bf)/(n-1 + 1e-12)\n\n@cython.boundscheck(False)\n@cython.wraparound(False)\ncpdef calculate_y_cython(li n):\n    cdef double[:] ys = np.empty(n, dtype=np.float64)\n    cdef:\n        int a = 2, b = 5, c = -4\n        li i = 0\n    for i in range(n):\n        ys[i] = y(i, n, a, b, c)\n    return ys\n```\n\nBefore building the object code, we need to change [build instructions to link Numpy](https://cython.readthedocs.io/en/stable/src/tutorial/numpy.html) as a dependency like following\n\n```python:setup.py\nfrom distutils.core import setup\nfrom distutils.extension import Extension\nfrom Cython.Build import cythonize\nimport numpy\n\next_modules = [\n    Extension(\"calculate_y_function\",\n              sources=[\"calculate_y_cython.pyx\"],\n              libraries=[\"m\"],\n              compiler_directives={\"language_level\": \"3\"},\n              )\n]\n\nsetup(name=\"calculate Y function\",\n      ext_modules=cythonize(ext_modules),\n      include_dirs=[numpy.get_include()])\n```\n\nThe parameter *include_dirs* includes external libraries like Numpy here.\n\n```python\n%%timeit -n 1000\nn = 100000\nres = calculate_y_cython(n)\n\n\"\"\"Output:\n208 µs ± 11.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\"\"\"\n```\n\nThe fully optimized version of Cython runs at **0.0002** seconds which is approximately **1000x** faster than the normal Python code. \n\nThe usage of Numpy and MemoryViews in Cython needs separate discussion and I will write about Classes, C-math, Numpy, MemoryViews, and OpenMP in the Advanced Cython series later.\n\n---\n\nCython is a very powerful extension that we can use to speed up Python code. Sometimes it may not be possible to convert Python objects straight away like dictionaries (use C++ maps), so it's better to use Cython for repetitive tasks like loops, general functions with simple statements (like declaration and usage only), and math operations.\n\nLearn more about Cython by referencing\n\n- [Speeding up basic object operations in Cython\n](http://blog.behnel.de/posts/tuning-basic-object-operations-in-cython.html)\n- [Faster Python made easier with Cython’s pure Python mode](https://www.infoworld.com/article/3648539/faster-python-made-easier-with-cythons-pure-python-mode.html)\n- [An Introduction to Just Enough Cython to be Useful](https://www.peterbaumgartner.com/blog/intro-to-just-enough-cython-to-be-useful/)\n- [Cython notes and tips](https://nicolas-hug.com/blog/cython_notes)\n- [(Github Issue) Improve cython interface with a more user-friendly compiling interface](https://github.com/cython/cython/issues/3974)"},{"metadata":{"title":"Super fast Python (Part-2): Good Practices","description":"Write Python programs by following good practices to run code incredibly faster.","imgName":"super-fast-python-good-practices/super-fast-python-good-practices.jpg","date":"Nov 9, 2022","tags":["python-performance"],"keywords":["python-performance","python-optimize","python","fast-python","speed","python"],"id":"super-fast-python-good-practices"},"content":"\n![Super fast Python: Good Practices](super-fast-python-good-practices/super-fast-python-good-practices.jpg)\n\n# Super fast Python (Part-2): Good practices\n\nIn the earlier post on [why Python is slow?](https://santhalakshminarayana.github.io/blog/super-fast-python-why-python-slow), we discussed slowness is in Python due to its internal design of some essential components like GIL, dynamic typing, and interpretation.\n\nIn this blog, we discuss some good practices to speed up Python incredibly faster.\n\nThis is the second post in the series on Python performance and Optimization. The series points out the utilization of inbuilt libraries, low-level code conversions, and other Python implementations to speed-up Python. The other posts included in this series are \n\n- (Part-1): [Why Python is slow?](https://santhalakshminarayana.github.io/blog/super-fast-python-why-python-slow)\n- (Part-2): Good practices to write fast Python code (this post)\n- (Part-3): [Multi-processing in Python](https://santhalakshminarayana.github.io/blog/super-fast-python-multi-processing)\n- (Part-4): [Use Cython to get speed as fast as C](https://santhalakshminarayana.github.io/blog/super-fast-python-cython)\n- (Part-5): [Use Numba to speed up Python Functions and Numeric calculations](https://santhalakshminarayana.github.io/blog/super-fast-python-numba)\n\nThe following section describes the various good practices one can use to make Python super speed (up to 30% or more) without any external support like PyPy, Cython, Numpy, etc.\n\n## Python good practices for super fast code\n\n### Use built-in data structures and libraries\n\nAs Python data types are implemented directly in **C**, using the built-in types like list, map, and trees, compared to custom types we define, really helps the program to run faster.\n\nAlso, use built-in libraries for common algorithms like counting the duplicates, summing all list elements, finding the maximum element, etc, because these are all already written in **C** and compiled which makes these functions run faster than custom functions we write.\n\n```python\nfrom random import randint\n\nrand_nums = [randint(1, 100) for _ in range(100000)]\n```\nCreate 100000 random numbers between 1 and 100.\n\n```python\n%%timeit\ncc = 0\nfor i in range(len(rand_nums)):\n    cc += rand_nums[i]\n```\n```python:output\n6.02 ms ± 94 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n```\n\nManually summing up the all numbers over running a loop takes approximately 6ms.\n\n```python\n%%timeit\ncc = sum(rand_nums)\n```\n```python:output\n332 µs ± 15 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n```\n\nUsing built-in *sum()* takes only 0.3ms approximately.\n\n### Local Variables vs Global Variables\n\nWhen we call a function (a routine), the system pauses the code execution at the call site in the current routine (say **main()**) where the call has been made and places the called function at the top of the call stack. Imagine if we have defined numerous global variables and made multiple function calls. The system has to make sure that all these global variables should be available for any routine placed in the call stack at all times. The system has to provide a lookup mechanism for both local and global variables for each routine. And with global variables, this lookup mechanism may take some time than local variables.\n\n### Import the sub-modules and functions directly\n\nWhen importing any module to use its sub-modules, classes, or functions, import them directly instead of importing just the module. When accessing objects using *.*, it triggers dictionary lookup using *\\_\\_getattribute\\_\\_*. If we call the object multiple times using *.*, that may increase the program time.\n\n```python\n# instead of this\nimport abc\ndef_obj = abc.Def()\n\n# do this\nfrom abc import Def\ndef_obj = Def()\n```\n\n### Limit the usage of '.'\n\nSpeaking of the lookup time with the module's object references, the same can be applied to the referencing of properties and functions of an object(both custom and in-built). \n\n```python\n%%timeit\nll = []\nfor i in range(len(rand_nums)):\n    ll.append(rand_nums[i])\n```\n```python:output\n6.67 ms ± 84.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n```\n\nAdding the list elements by appending with *list.append()* takes more time than following code because the function is assigned to a variable (functions are first-class citizens in Python) and used inside the loop. This simple practice avoids referencing the functions with **'.'** too often and finally limits the need for dictionary lookup.\n\n```python\n%%timeit\nll = []\nll_append = ll.append\nfor i in range(len(rand_nums)):\n    ll_append(rand_nums[i])\n```\n```python:output\n5.45 ms ± 116 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n```\n\n### Avoid writing functions unnecessary\n\nIt's good to have code separability by using functions for each independent task. But, as functions in Python are relatively more expensive than C/C++ due to boxing and unboxing dynamic variables and other factors, limit writing functions for unnecessary cases like one-liners. \n\n### Don't wrap lambdas around functions\n\nOveruse or misuse of lambdas is not a good practice. It's common to wrap functions inside lambdas which do the same thing without wrapping.\n\nConsider the following two functions for sorting a list based on absolute values.\n\n```python\ndef fun_sort_with_lambda(l):\n    return sorted(l, key=lambda x: abs(x))\n\ndef fun_sort_without_lambda(l):\n    return sorted(l, key=abs)\n```\n\nIf we look at the CPython bytecode for the above functions with lambda expression passed as a key and with *abs* function object as a key,\n\n```python\n\u003e\u003e\u003e from dis import dis\n\u003e\u003e\u003e dis(fun_sort_with_lambda)\n  2           0 LOAD_GLOBAL              0 (sorted)\n              2 LOAD_FAST                0 (l)\n              4 LOAD_CONST               1 (\u003ccode object \u003clambda\u003e at 0x7fc51b3a19d0, file \"\u003cipython-input-62-c4147c242c71\u003e\", line 2\u003e)\n              6 LOAD_CONST               2 ('fun_sort_with_lambda.\u003clocals\u003e.\u003clambda\u003e')\n              8 MAKE_FUNCTION            0\n             10 LOAD_CONST               3 (('key',))\n             12 CALL_FUNCTION_KW         2\n             14 RETURN_VALUE\n\nDisassembly of \u003ccode object \u003clambda\u003e at 0x7fc51b3a19d0, file \"\u003cipython-input-62-c4147c242c71\u003e\", line 2\u003e:\n  2           0 LOAD_GLOBAL              0 (abs)\n              2 LOAD_FAST                0 (x)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n\u003e\u003e\u003e dis(fun_sort_without_lambda)\n  2           0 LOAD_GLOBAL              0 (sorted)\n              2 LOAD_FAST                0 (l)\n              4 LOAD_GLOBAL              1 (abs)\n              6 LOAD_CONST               1 (('key',))\n              8 CALL_FUNCTION_KW         2\n             10 RETURN_VALUE\n```\n\nfor the function *fun_sort_with_lambda*, there is an additional function has been generated for lambda. We can avoid this function generation without using lambda as we can see in function *fun_sort_without_lambda*. \n\n### List comprehension is fast\n\nWhen operating over lists like data structures, list comprehension is faster than traditional methods like looping, functional programming, etc.\n\n```python\n%%timeit\nrand_nums = []\nfor _ in range(1000):\n    rand_nums.append(randint(1, 100))\n```\n```output\n603 µs ± 15.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n```\n\nThe list comprehension version of the above code snippet runs faster.\n\n```python:output\n%%timeit\nrand_nums = [randint(1, 100) for _ in range(1000)]\n```\n```python:output\n565 µs ± 11 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n```\n\n---\n\nThe optimization practices are not limited to the above approaches. We can check how much time it is taking for each line by using libraries like [CProfile](https://docs.python.org/3/library/profile.html) and making changes to run faster. In the next blog, we discuss how to improve Python computing efficiency using multiprocessing.\n\n---\n\n### References\n- [Python Speed](https://wiki.python.org/moin/PythonSpeed)\n- [Python Optimization](https://aglowiditsolutions.com/blog/python-optimization/)\n- [Making Python Programs Blazingly Fast](https://martinheinz.dev/blog/13)"},{"metadata":{"title":"Super fast Python (Part-1): Why Python is Slow?","description":"Why Python is slow compared to C/C++ or Java? And where can we improve our code to run fast?","imgName":"super-fast-python-why-python-slow/super-fast-python-why-python-slow.jpg","date":"Nov 7, 2022","tags":["python-performance"],"keywords":["python-performance","python-optimize","python","fast-python","speed","python","slow-python"],"id":"super-fast-python-why-python-slow"},"content":"\n![Super fast Python: Why Python is Slow?](super-fast-python-why-python-slow/super-fast-python-why-python-slow.jpg)\n\n# Super fast Python (Part-1): Why Python is Slow?\n\nPython is an interpreted, high-level, and dynamically typed programming language. Developers prefer Python because of its easy-to-learn, flexibility, fast development, easy-to-code, readability, and many other development choices. From nowhere in the 2000s to the most used programming language right now, Python has come a long way with help of a strong community. But, there has been always a discussion on Python's choice for computation-intensive tasks like Machine Learning as Python is generally slow compared to widely used languages like C/C++ and Java. Even with the development of computation-efficient libraries and packages like Numpy, still Python is slow for general usage. \n\nPython's core development team has been working to make Python as fast as C/C++. They set a goal to make each Python release significantly faster than the earlier release. The current [Python 3.11 is up to 10-60 percent faster than Python 3.10](https://docs.python.org/3.11/whatsnew/3.11.html#faster-cpython). Let's hope we will reach a state where Python is at least at the same speed level as Java if not C++. \n\nOne common practice to tackle the speed issues in production is to upgrade the hardware resources or upscale the cloud infrastructure that increases the project budget. As Python's core development team is trying to improve Python, it's up to us to leverage the core libraries and code practices to make code faster at the developer's end and eventually use fewer resources and budget.\n\nThis is the first post in the series on Python performance and Optimization. The series points out the utilization of inbuilt libraries, low-level code conversions, and other Python implementations to speed-up Python. The other posts included in this series are \n\n- (Part-1): Why Python is slow? (this post)\n- (Part-2): [Good practices to write fast Python code](https://santhalakshminarayana.github.io/blog/super-fast-python-good-practices)\n- (Part-3): [Multi-processing in Python](https://santhalakshminarayana.github.io/blog/super-fast-python-multi-processing)\n- (Part-4): [Use Cython to get speed as fast as C](https://santhalakshminarayana.github.io/blog/super-fast-python-cython)\n- (Part-5): [Use Numba to speed up Python Functions and Numeric calculations](https://santhalakshminarayana.github.io/blog/super-fast-python-numba)\n\n---\n\n## Why Python is slow?\n\nBefore looking into how we can optimize the Python code, we should look at first why Python is slow. \n\nSome of the reasons for Python's slowness is due to its design of core details like how code executes, type-inference, and memory management. \n\n### Python implementation by interpretation\n\nPython is a programming language that talks about syntax and rules to write programs. Executing the code written is done by the [programming language implementation](https://en.wikipedia.org/wiki/Programming_language_implementation). This can be of two categories - Compilation and Interpretation. **CPython** is a Python implementation written in **C** that applies an interpretation approach to execute the python code written. CPython is the default runtime and reference implementation of Python and there are other runtimes like PyPy, Cython, Jython, etc., that take different execution approaches for different use cases. CPython is both an interpreter (widely represented) and a compiler as it complies the Python code to Python bytecode and then interprets it for the specific platform using Python Virtual Machine (PVM).\n\nCPython uses Global Interpreter Lock (GIL) on each CPython interpreter process. This means, within a single process, only a single thread processes the Python bytecode. We will see later why this behavior is both good and bad.\n\nAs interpretation is usually slow compared to compilation it is understood that Python is slow, but maybe up to 2x-3x times slower than C/C++. But it is not true, Java, which also interprets the code, is still many times faster than Python and this asks questions on what are other factors for slowness in Python. \n\nChanging the runtime of Python from interpretation to compilation like Cython and embedded C-code, or JIT with PyPy, we can improve the speed of Python many times. We will talk about Cython, and how to use it with Python, in a later article.\n\n### Comes the dynamically typing comes the problem \n\nOne of the beautiful features of Python is dynamic typing and many people like the way it is. But with dynamic typing, there is an additional burden on the interpreter to keep track of the type of variables that makes less scope for optimization. As Java is statically typed, the interpreter can optimize the bytecode generation and can interpret it faster than Python. \n\n```python\na = 1 # a as int\nb = a * 2 # b as int \n\na = 'python ' # a as string\n# c as a string with operations\n# of string and int\nc = a * b \n```\n\nIn the above snippet, the interpreter has to keep track of the *type* of **a** from top to bottom. If not dynamic typing is supported in Python, we have to declare types for every variable and the operation **c = a * b** is not possible then. So, with flexible support from Python, there is some overhead with the interpreter too.\n\n### That is an object, this too\n\nIn Python, everything is an object. Even functions too. If one can remember how objects are referenced in C++ (yes, the pointers), apply the same concept in Python for everything including built-in primitive (not exactly) types which are objects too and are specially taken care of. \n\nAssigning memory to the objects is done by creating actual memory to hold an object and a reference is given to the variables that point to the real object in the memory. Every time the variable changes its value, instead of changing the value in the memory, a new object is created and the variable is given the new location of the object that holds the new value. As CPython is implemented in C, objects created in Python are called PyObjects which are struct in C that refers to all Python objects.\n\n```python\n\u003e\u003e\u003e a = 10\n\u003e\u003e\u003e print(id(a))\n9801536\n\n\u003e\u003e\u003e a = 12\n\u003e\u003e\u003e print(id(a))\n9801600\n``` \n\nWe can see the address of **a** changes every time we change/assign the value because of how Python manages variables and their values. In C/C++, for variables, the addressing is done by creating an actual location for the variable instead of the value, and if the value changes, it just overwrites the previous value.\n\nPython has to create new objects, keep their references, delete unused objects, and repeat the cycle. This continuous cycle of object creation and deletion whenever variable values changes make runtime slow.\n\nThe major overhead that needs to address is Python's way of handling collections like Lists. In *C*, we create the arrays with fixed memory and the address of that array is fixed, and continuous memory allocation happens for all the elements starting from the fixed memory point. But in Python, for each value, an [object is created](http://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/) anywhere in the memory (not continuous), and the list holds only references to these objects. Working on these arbitrary memory locations make things complex and eventually takes more time.\n\n```python\n\u003e\u003e\u003e l = [1, 2, 3, 4]\n\u003e\u003e\u003e for i in l: \n        print(id(i))\n\nOUTPUT:\n-------\n\n9801248\n9801280\n9801312\n9801344\n```\n\nIn the above snippet, we can observe that the addresses for each element in the list are not the same or continuous.\n\nOne of the reasons why Numpy is faster is because it creates fixed memory of the array elements. The below snippet prints out the memory address of each element in the array and they are the same, meaning Numpy create an array with continuous memory address and they are easy to operate.\n\n```python\n\u003e\u003e\u003e a = np.array([1, 2, 3, 4])\n\u003e\u003e\u003e for i in a: \n        print(i.__array_interface__['data'][0]) \n\nOUTPUT:\n-------\n\n37936064\n37936064\n37936064\n37936064\n```\n\n\n### GIL, good and bad:=:gil-good-and-bad\n\nIn **C**, there is no inbuilt support for garbage collection. One has to manually de-allocate/free the memory. Unlike C, Python does garbage collection by using a mechanism called reference count. For every PyObject, Python keeps track of a count of how many references are pointing to the current object. If no reference is pointing to the current object, then, Python frees the memory by deleting the unused object. \n\nEarlier we talked about GIL that, in the current process, GIL locks the process to have only a single thread to run the Python interpreter. This does not mean we cannot use more threads, but it is hard and takes more time compared to multi-threading in other languages because CPython creates a separate environment for each thread. This limits multi-threading in Python to use only in situations where threads wait for external processes to complete like I/O operations, network requests, etc. \n\nWhen multiple threads access the same resource, it is hard to keep track of the reference count of objects. GIL is required in CPython to ensure thread-safe as it allows only a single thread to process. As Python is a general-purpose language, in most cases there is no need to implement multi-threading, and there is no problem with GIL.\n\nBut when there is a need to optimize CPU-intensive tasks, one must use multi-processing to utilize all cores and reduce the overall speed. We will talk about different use cases for multi-threading and multi-processing in the later part of this series.\n\n---\n\nNow, we have learned that there are a lot of things that contribute to the slowness of Python compared to other languages. We cannot change the design of Python but we can change how we write the code by using good libraries, code snippets, and changing implementations. \n\nIn the upcoming posts, we discuss how we can use both internal and external libraries like *multiprocessing*, *Cython*, and *Numba* to make Python blazingly faster."},{"metadata":{"title":"What is React Native?","date":"Mar 23, 2022","excerpt":"Lorem, ipsum dolor sit amet consectetur adipisicing elit. Ea deserunt ab maiores eligendi nemo, ipsa, pariatur","id":"what-is-react-native"},"content":"\n# What is React Native?\n\nReact Native is a lovely language\n\n### React Native App Example:\n\n```js\nconst App = () =\u003e {\n  return (\n    \u003cView\u003e\n      \u003cText\u003eHello World Example\u003c/Text\u003e\n    \u003c/View\u003e\n  );\n};\n\nexport default App;\n```\n"}],"layout":"main"},"__N_SSG":true},"page":"/","query":{},"buildId":"DPwb1nK7hmgB_yIPdLqpw","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>